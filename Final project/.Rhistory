docs <- tm_map(docs, segmentCN, nature = TRUE)
library("tmcn")
docs <- tm_map(docs, segmentCN, nature = TRUE)
library("tmcn")
library("rJava")
install.packages("rJava")
install.packages("Rwordseg")
install.packages("SnowballC")
install.packages("slam")
install.packages("slam")
install.packages("slam")
install.packages("slam")
library("tmcn")
library("rJava")
library("Rwordseg")
library("rJava")
library("Rwordseg")
library("SnowballC")
library("slam")
token <- "EAACEdEose0cBAIZCUie4VvGOYGCUZAl9ZB6CVUK7bRZCegWFHZCgHLO6DQNBIyC5VWrDWlFzE2lNwykfvV2ecgm82uNkfZAQZB4QgbUcUZAjzRMqrAJIFRQZCsppqt7fZC0ZCX1Vsp92NiiJ5TfMq62Ulis194ZCnh7F4S4z52t80RTtuiVdhXukfOXsEMrZBp2uvIM9CkCmS3ZCdj8AZDZD"
page.id <- "382641298513460"
page <- getPage(page.id, token, n = 50)
Page <- subset(page, select = c(from_id, message))
colnames(Page) <- c("doc_id","text")
print(Page$text[1])
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
docs <- VCorpus(DataframeSource(Page))
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb1\x8d')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb1\x89')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\x8d')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\xb1')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbf\x83')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb4\x9c')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbd\xbd')
docs <- tm_map(docs, toSpace, '\u2b50')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\xad')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\x8b')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbe\xed\xb6\x80')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbd\xa5')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\x82')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbd\xb2')
docs<-tm_map(docs,stripWhitespace)
docs<-tm_map(docs,removeNumbers)
docs<-tm_map(docs,removePunctuation)
for (i in c(1:50)){
print(docs[[i]][1])
}
docs <- tm_map(docs, segmentCN, nature = TRUE)
tdm <- TermDocumentMatrix(docs, control = list(wordLengths = c(1,1)))
inspect(tdm)
library(magrittr)
library(devtools)
library(Rfacebook)
library("jiebaR")
library(tm)
library(NLP)
library("tmcn")
library("rJava")
library("Rwordseg")
install.packages("rJava")
library("rJava")
cc = worker()
library(magrittr)
library(devtools)
library(Rfacebook)
library("jiebaR")
library(tm)
library(NLP)
library("tmcn")
library("SnowballC")
library("slam")
token <- "EAACEdEose0cBAIZCUie4VvGOYGCUZAl9ZB6CVUK7bRZCegWFHZCgHLO6DQNBIyC5VWrDWlFzE2lNwykfvV2ecgm82uNkfZAQZB4QgbUcUZAjzRMqrAJIFRQZCsppqt7fZC0ZCX1Vsp92NiiJ5TfMq62Ulis194ZCnh7F4S4z52t80RTtuiVdhXukfOXsEMrZBp2uvIM9CkCmS3ZCdj8AZDZD"
page.id <- "382641298513460"
page <- getPage(page.id, token, n = 50)
Page <- subset(page, select = c(from_id, message))
colnames(Page) <- c("doc_id","text")
print(Page$text[1])
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
docs <- VCorpus(DataframeSource(Page))
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb1\x8d')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb1\x89')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\x8d')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\xb1')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbf\x83')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb4\x9c')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbd\xbd')
docs <- tm_map(docs, toSpace, '\u2b50')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\xad')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\x8b')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbe\xed\xb6\x80')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbd\xa5')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbd\xed\xb8\x82')
docs <- tm_map(docs, toSpace, '\xed\xa0\xbc\xed\xbd\xb2')
docs<-tm_map(docs,stripWhitespace)
docs<-tm_map(docs,removeNumbers)
docs<-tm_map(docs,removePunctuation)
for (i in c(1:50)){
print(docs[[i]][1])
}
cc = worker()
tdm <- TermDocumentMatrix(cc[docs], control = list(wordLengths = c(1,1)))
inspect(tdm)
cc[docs]
myfunc <- lapply( "https://goo.gl/Pi1uzn", source)
CheckMyHw( "B05902010", "HW01", "BIG5" )
CheckMyHw( "B05902010", "HW02", "BIG5" )
CheckMyHw( "B05902010", "HW04", "BIG5" )
CheckMyHw( "B05902010", "HW03", "BIG5" )
kable(head(GATDF))
library(knitr)
kable(head(GATDF))
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
data <- list()
title <- list()
#原本用getURL 結果發現url.list裡面是NULL 改用GET後就正確了
#抓取從明星賽後到現在的 ptt NBA版的貼文
for( i in c(5674:5810)){
tmp <- paste(i, '.html', sep='')
url <- paste('www.ptt.cc/bbs/NBA/index', tmp, sep='')
html <- htmlParse(GET(url),encoding = "UTF-8")
title.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlValue)
url.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlAttrs)
data <- rbind(data, paste('www.ptt.cc', url.list, sep=''))
title <- rbind(title, title.list)
}
# data 存網址  title 存標題
data <- unlist(data)
title <- unlist(title)
#初始化
Lebron <- c()
Giannis <- c()
Curry <- c()
Harden <- c()
Lebron.url <- c()
Giannis.url <- c()
Curry.url <- c()
Harden.url <- c()
# 找出有關鍵字的標題並分類
lbj1 <- grep("姆斯",title)
lbj2 <- grep("James",title)
lbj3 <- grep("LBJ",title)
lbj4 <- grep("LeBron",title)
curry1 <- grep("Curry", title)
curry2 <- grep("柯瑞", title)
harden1 <- grep("Harden", title)
harden2 <- grep("哈登", title)
giannis1 <- grep("Giannis", title)
giannis2 <- grep("字母", title)
Lebron <- c(Lebron,title[lbj1])
Lebron <- c(Lebron,title[lbj2])
Lebron <- c(Lebron,title[lbj3])
Lebron <- c(Lebron,title[lbj4])
Lebron.url <- c(Lebron.url,data[lbj1])
Lebron.url <- c(Lebron.url,data[lbj2])
Lebron.url <- c(Lebron.url,data[lbj3])
Lebron.url <- c(Lebron.url,data[lbj4])
Curry <- c(Curry,title[curry1])
Curry <- c(Curry,title[curry2])
Curry.url <- c(Curry.url,data[curry1])
Curry.url <- c(Curry.url,data[curry2])
Harden <- c(Harden,title[harden1])
Harden <- c(Harden,title[harden2])
Harden.url <- c(Harden.url, data[harden1])
Harden.url <- c(Harden.url, data[harden2])
Giannis <- c(Giannis,title[giannis1])
Giannis <- c(Giannis,title[giannis2])
Giannis.url <- c(Giannis.url, data[giannis1])
Giannis.url <- c(Giannis.url, data[giannis2])
message <- list()
cc = worker()
LBJTDF <- data.frame()
SCTDF <- data.frame()
GATDF <- data.frame()
JHTDF <- data.frame()
for(i in c(1:length(Giannis))){
html <- htmlParse(GET(Giannis.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
d.corpus <- VCorpus( VectorSource(message) )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
abc <- data.frame(table(cc[as.character(d.corpus)]))
colnames(abc) <- c("word", as.character(i))
if(i == 1){
GATDF <- abc}
else{
GATDF <- merge(GATDF, abc, by = "word", all = T)}
}
GATDF[is.na(GATDF)] <- 0
library(knitr)
kable(head(GATDF))
n <- length(Giannis)
tf <- apply(as.matrix(GATDF[,2:(n+1)]), 2, sum)
library(Matrix)
idfCal <- function(word_doc)
{
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(GATDF[,2:(n+1)]), 1, idfCal)
doc.tfidf <- GATDF
for(x in 1:nrow(GATDF))
{
for(y in 2:ncol(GATDF))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
kable(head(GATDF))
View(doc.tfidf)
kable(head(doc.tfidf))
View(doc.tfidf)
kable(head(doc.tfidf$`7`))
kable(head(arrange(doc.tfidf,desc(8))))
library(dplyr)
kable(head(arrange(doc.tfidf,desc(8))))
kable(head(arrange(doc.tfidf,desc("8"))))
arrange(doc.tfidf,desc(8)))
arrange(doc.tfidf,desc(8))
order(doc.tfidf$`6`, decreasing = TRUE)
View(doc.tfidf)
doc.tfidf[ order(doc.tfidf$`6`, decreasing = TRUE), ]
head(doc.tfidf[order(doc.tfidf$`6`, decreasing = TRUE), ])
View(doc.tfidf)
myfunc <- lapply( "https://goo.gl/Pi1uzn", source)
CheckMyHw( "B01901369", "HW01", "BIG5" )
CheckMyHw( "B05902010", "HW01", "BIG5" )
CheckMyHw( "B05902010", "HW03", "BIG5" )
CheckMyHw( "B05902040", "HW03", "BIG5" )
CheckMyHw( "B05902111", "HW03", "BIG5" )
CheckMyHw( "B05902115", "HW03", "BIG5" )
CheckMyHw( "B05902116", "HW03", "BIG5" )
myfunc <- lapply( "https://goo.gl/Pi1uzn", source)
CheckMyHw( "B05902010", "HW04", "BIG5" )
CheckMyHw( "B05902040", "HW04", "BIG5" )
CheckMyHw( "B05902111", "HW04", "BIG5" )
CheckMyHw( "B05902115", "HW04", "BIG5" )
CheckMyHw( "B05902116", "HW04", "BIG5" )
library(rvest)
library(XML)
library(magrittr)
all_data <- matrix(ncol=10,nrow=0)
for (tyear in 2013:2017){
stockURL <- paste("http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr=",tyear,"&p=.htm",sep="")
sdat1 <- html(stockURL)  %>%
html_nodes("table") %>%
extract2(4) %>%
html_nodes("td") %>%
html_text()
temp <- matrix(sdat1[15:914],nrow=100,byrow = TRUE)
all_temp <- cbind(rep(tyear,100),temp)
all_data <- rbind(all_data,all_temp)
}
library(rvest)
library(XML)
library(magrittr)
all_data <- matrix(ncol=10,nrow=0)
for (tyear in 2013:2017){
stockURL <- paste("http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr=",tyear,"&p=.htm",sep="")
sdat1 <- html(stockURL)  %>%
html_nodes("table") %>%
extract2(4) %>%
html_nodes("td") %>%
html_text()
temp <- matrix(sdat1[15:914],nrow=100,byrow = TRUE)
all_temp <- cbind(rep(tyear,100),temp)
all_data <- rbind(all_data,all_temp)
}
BoxofficeDF <- data.frame(all_data)
colnames(BoxofficeDF) <- c("Year","Yearly_Ranking","Title","Studio","Total_Gross","Number_of_Theaters","Opening_Gross","Number_of_Opening_Theaters","Open_Date","End_Date")
library(XML)
library(rvest)
library(dplyr)
#爬取30支球隊全壘打的數量
movie <- read_html("http://www.boxofficemojo.com/weekend/chart/?yr=2018&wknd=18&p=.htm")
movies <- c("td td")
#爬取30支球隊全壘打的數量
movie <- read_html("http://www.boxofficemojo.com/weekend/chart/?yr=2018&wknd=18&p=.htm")
movies <- c("td td")
moviess <- html_nodes(movie,movies)
moviess <- html_text(moviess)
moviess <- iconv(moviess,"UTF-8")
moviess
url <- 'http://www.boxofficemojo.com/weekend/chart/?yr=2018&wknd=18&p=.html'
html <- htmlParse(GET(url),encoding = "UTF-8")
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
url <- 'http://www.boxofficemojo.com/weekend/chart/?yr=2018&wknd=18&p=.html'
html <- htmlParse(GET(url),encoding = "UTF-8")
title.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlValue)
title.list <- xpathSApply(html, "//td[@align='right']/b", xmlValue)
html <- htmlParse(GET(url),encoding = "UTF-8")
url <- 'http://www.boxofficemojo.com/weekend/chart/?yr=2018&wknd=18&p=.htm'
html <- htmlParse(GET(url),encoding = "UTF-8")
title.list <- xpathSApply(html, "//td[@align='right']/b", xmlValue)
html <- htmlParse(GET(url),encoding = "UTF-8")
url <- 'http://www.boxofficemojo.com/weekend/chart/?yr=2018&wknd=18&p=.htm'
html <- htmlParse(GET(url),encoding = "UTF-8")
title.list <- xpathSApply(html, "//td[@align='right']/b", xmlValue)
url.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlAttrs)
date.list <- xpathSApply(html, "//div[@class='meta']/div[@class='date']", xmlValue)
data <- rbind(data, as.matrix(paste('www.ptt.cc', url.list, sep='')))
title <- rbind(title, as.matrix(title.list))
library(XML)
url <- 'http://www.boxofficemojo.com/weekend/chart/?yr=2018&wknd=18&p=.htm'
dt1 <- readHTMLTable(url,header = T)
names(dt1[[1]]) <- rvest::repair_encoding(names(dt1[[1]]))
dt1 <- readHTMLTable(url,header = T)
names(dt1[[1]]) <- rvest::repair_encoding(names(dt1[[1]]))
View(dt1)
names(dt1[[2]]) <- rvest::repair_encoding(names(dt1[[2]]))
head(dt1[[1]])
head(dt1[[3]])
head(dt1[[4]])
head(dt1[[5]])
dt1[[5]]
names(dt1[[5]]) <- rvest::repair_encoding(names(dt1[[5]]))
dt1[[5]]
names(dt1[[5]]) <- rvest::repair_encoding(names(dt1[[5]]))
dt1[[5]]
library(rvest)
library(rvest)
library(XML)
library(magrittr)
all_data <- matrix(ncol=10,nrow=0)
for (tyear in 2013:2017){
stockURL <- paste("http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr=",tyear,"&p=.htm",sep="")
sdat1 <- html(stockURL)  %>%
html_nodes("table") %>%
extract2(4) %>%
html_nodes("td") %>%
html_text()
temp <- matrix(sdat1[15:914],nrow=100,byrow = TRUE)
all_temp <- cbind(rep(tyear,100),temp)
all_data <- rbind(all_data,all_temp)
}
BoxofficeDF <- data.frame(all_data)
colnames(BoxofficeDF) <- c("Year","Yearly_Ranking","Title","Studio","Total_Gross","Number_of_Theaters","Opening_Gross","Number_of_Opening_Theaters","Open_Date","End_Date")
write.table(BoxofficeDF,file="/Users/Eric/Documents/R study/Final Project/BoxofficeDF.csv",row.names=FALSE,sep=",")
setwd("~/Desktop/cs-x-programming/Final project")
all_data <- matrix(ncol=10,nrow=0)
for (tyear in 2013:2017){
stockURL <- paste("http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr=",tyear,"&p=.htm",sep="")
sdat1 <- html(stockURL)  %>%
html_nodes("table") %>%
extract2(4) %>%
html_nodes("td") %>%
html_text()
temp <- matrix(sdat1[15:914],nrow=100,byrow = TRUE)
all_temp <- cbind(rep(tyear,100),temp)
all_data <- rbind(all_data,all_temp)
}
BoxofficeDF <- data.frame(all_data)
colnames(BoxofficeDF) <- c("Year","Yearly_Ranking","Title","Studio","Total_Gross","Number_of_Theaters","Opening_Gross","Number_of_Opening_Theaters","Open_Date","End_Date")
View(BoxofficeDF)
require(arules)
str(BoxofficeDF)
View(BoxofficeDF)
View(BoxofficeDF)
rule <- apriori(BoxofficeDF,
# min support & confidence, 最小規則長度(lhs+rhs)
parameter=list(minlen=3, supp=0.1, conf=0.7),
appearance = list(default="lhs",
rhs=c("Year=2013", "Year=2014")
# 右手邊顯示的特徵
)
)
require(arules)
rule <- apriori(BoxofficeDF,
# min support & confidence, 最小規則長度(lhs+rhs)
parameter=list(minlen=3, supp=0.1, conf=0.7),
appearance = list(default="lhs",
rhs=c("Year=2013", "Year=2014")
# 右手邊顯示的特徵
)
)
install.packages("arules")
require(arules)
install.packages("arules")
install.packages("arules")
rule <- apriori(BoxofficeDF,
# min support & confidence, 最小規則長度(lhs+rhs)
parameter=list(minlen=3, supp=0.1, conf=0.7),
appearance = list(default="lhs",
rhs=c("Year=2013", "Year=2014")
# 右手邊顯示的特徵
)
)
require(arules)
rule <- apriori(BoxofficeDF,
# min support & confidence, 最小規則長度(lhs+rhs)
parameter=list(minlen=3, supp=0.1, conf=0.7),
appearance = list(default="lhs",
rhs=c("Year=2013", "Year=2014")
# 右手邊顯示的特徵
)
)
inspect(rule)
sort.rule <- sort(rule, by="lift")
inspect(sort.rule)
subset.matrix <- as.matrix(is.subset(x=sort.rule, y=sort.rule))
subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
redundant <- colSums(subset.matrix, na.rm=T) >= 1
sort.rule <- sort.rule[!redundant]
inspect(sort.rule)
require(arulesViz)
plot(sort.rule)
install.packages("arulesViz")
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph", control=list(type="items"))
plot(sort.rule, method="grouped")
install.packages("arulesViz")
require(arulesViz)
plot(sort.rule)
install.packages("arulesViz")
install.packages("dendextend")
install.packages("arulesViz")
install.packages("robustbase")
install.packages("robustbase")
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph", control=list(type="items"))
rm(list=ls())
str(BoxofficeDF)
install.packages("arules")
install.packages("arules")
require(arules)
rule <- apriori(BoxofficeDF,
# min support & confidence, 最小規則長度(lhs+rhs)
parameter=list(minlen=3, supp=0.1, conf=0.7),
appearance = list(default="lhs",
rhs=c("Year=2013", "Year=2014")
# 右手邊顯示的特徵
)
)
library(rvest)
library(XML)
library(magrittr)
all_data <- matrix(ncol=10,nrow=0)
for (tyear in 2013:2017){
stockURL <- paste("http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr=",tyear,"&p=.htm",sep="")
sdat1 <- html(stockURL)  %>%
html_nodes("table") %>%
extract2(4) %>%
html_nodes("td") %>%
html_text()
temp <- matrix(sdat1[15:914],nrow=100,byrow = TRUE)
all_temp <- cbind(rep(tyear,100),temp)
all_data <- rbind(all_data,all_temp)
}
BoxofficeDF <- data.frame(all_data)
colnames(BoxofficeDF) <- c("Year","Yearly_Ranking","Title","Studio","Total_Gross","Number_of_Theaters","Opening_Gross","Number_of_Opening_Theaters","Open_Date","End_Date")
str(BoxofficeDF)
install.packages("arules")
install.packages("arules")
require(arules)
rule <- apriori(BoxofficeDF,
# min support & confidence, 最小規則長度(lhs+rhs)
parameter=list(minlen=3, supp=0.1, conf=0.7),
appearance = list(default="lhs",
rhs=c("Year=2013", "Year=2014")
# 右手邊顯示的特徵
)
)
inspect(rule)
sort.rule <- sort(rule, by="lift")
inspect(sort.rule)
subset.matrix <- as.matrix(is.subset(x=sort.rule, y=sort.rule))
subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
redundant <- colSums(subset.matrix, na.rm=T) >= 1
sort.rule <- sort.rule[!redundant]
inspect(sort.rule)
sort.rule <- sort.rule[!redundant]
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph", control=list(type="items"))
library(arulesViz)
install.packages("arulesViz")
detach("package:arules", unload=TRUE)
library("arules", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
require(arulesViz)
install.packages("arulesViz")
install.packages("dendextend")
install.packages("robustbase")
install.packages("arulesViz")
install.packages("dendextend")
install.packages("robustbase")
gsub("$", "",a)
a <- "$ajidfoajpfia"
gsub("$", "",a)
